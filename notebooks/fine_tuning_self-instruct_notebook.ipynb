{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e0e3e2",
   "metadata": {},
   "source": [
    "## LLM fine tuning with self-instruct datasets\n",
    "\n",
    "\n",
    "This notebook provides a comprehensive guide for fine tuning the Mistral-7B-instruct model with LoRA, using the training/validation datasets synthetically generated from the self-instruct framework.  \n",
    "     \n",
    "This is the step 5 of the self-instruct framework, continuing from the \"synthetic_data_generation_self-instruct_notebook\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36330086",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c044bf-5cc7-4154-84b1-1911ee2ae195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install the required Python packages \n",
    "\n",
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cc815-2012-427f-ae5d-4551671c9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb64de6-aa77-469a-bd9d-66c47f3fac8b",
   "metadata": {},
   "source": [
    "### Load the synthetic datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcdda9",
   "metadata": {},
   "source": [
    "Define prompt keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac3fc5-2f3d-4950-ab00-0e77faa49419",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_KEY = \"<s>[INST]\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"[/INST]\"\n",
    "END_KEY = \"</s>\"\n",
    "#RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "# This is a training prompt that does not contain an input string.  The instruction by itself has enough information\n",
    "# to respond.  For example, the instruction might ask for the year a historic figure was born.\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{instruction_key}{instruction}{response_key}{response}{end_key}\"\"\".format(\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is a training prompt that contains an input string that serves as context for the instruction.  For example,\n",
    "# the input might be a passage from Wikipedia and the intruction is to extract some information from it.\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{instruction_key}{instruction}{input_key}{input}{response_key}{response}{end_key}\"\"\".format(\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    input=\"{input}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778b4e9",
   "metadata": {},
   "source": [
    "Prepare training data in Datasets format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33b402-b7f5-4589-9b69-23f5162f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "html_file = <your document.html> \n",
    "TRN_FILE = html_file+\"_trnfromseed_mistral_20240505.csv\"  \n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=TRN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fc930-2588-4a45-aefa-3eaa30cfb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "        instruction = rec[\"question\"]\n",
    "        response = rec[\"answer\"]\n",
    "        context = rec.get(\"context\")\n",
    "        \n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=(instruction.replace('\\n','')).replace('  ',''),\n",
    "                                                    response=(response.replace('\\n','')).replace('  ','')+'  [STOP][STOP]'\n",
    "                                                   ).strip()\n",
    "\n",
    "        return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e2677-5762-444a-bb4c-be2690aaa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prompt keys to dataset\n",
    "dataset = dataset.map(_add_text)\n",
    "\n",
    "# Remove data fields not needed \n",
    "dataset = dataset.map(\n",
    "        batched=True,\n",
    "        remove_columns=['context', 'seed_question', 'question', 'answer'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9777454-403a-41b1-9c45-3bfbaf0d855d",
   "metadata": {},
   "source": [
    "### LLM Fine tuning\n",
    "\n",
    "Here we use the Mistral-7b-instruct model for PEFT/LoRA based fine-tuning. Load the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba182a1a-3819-4452-9dfa-cad2cceb40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "OUTPUT_DIR = 'models/mistral-ft-doc-d2lai-self-instruct-20240505'\n",
    "\n",
    "device_map=\"auto\"\n",
    "TOKEN = <your token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(      \n",
    "    BASE_MODEL, \n",
    "    torch_dtype=torch.bfloat16,             \n",
    "    device_map=device_map,\n",
    "    token = TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  \n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aee3e4-2ee1-4f0f-86d7-acb3c42bddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    token = TOKEN,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc4157",
   "metadata": {},
   "source": [
    "Set LoRA and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57df3d9-81f0-47d1-80a2-0b7f6dbc6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=256,                   \n",
    " lora_alpha=64,                        \n",
    " lora_dropout=0.05, \n",
    " target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\"], \n",
    " bias=\"none\",\n",
    " task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d94157-d0cc-4bfa-ba7b-148de58e1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(    \n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=5e-6,\n",
    "        num_train_epochs=3,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=20,   \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20000,\n",
    "        save_total_limit=10,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4d91d-01a1-44a9-abef-8769f22f40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'],\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac5f7-d82c-4f12-b5be-9fb2c6620ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Training time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23894d8c-74dc-4de2-afe8-415580d39cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained LoRA adapter\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f9f4e",
   "metadata": {},
   "source": [
    "### Inference the fine-tuned model using transformers pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer = tokenizer, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17007174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def Mistral_Infer(query):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    sequences = pipe(\n",
    "        f\"<s>[INST] {query} [/INST]\",\n",
    "        do_sample=True,\n",
    "        max_new_tokens=512, \n",
    "        temperature=0.05, \n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    full_text = sequences[0]['generated_text'].split('[STOP][STOP]')[0]\n",
    "    answer = full_text.split('[/INST]')[1]\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1586488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "VAL_FILE = html_file+\"_valfromseed_mistral-7b.csv\"  \n",
    "\n",
    "df_val = pd.read_csv (VAL_FILE)\n",
    "question_list = df_val.question.values.tolist()\n",
    "ref_answer_list = df_val.answer.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21552ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX=5\n",
    "query = question_list[IDX]  \n",
    "ref_answer = ref_answer_list[IDX]\n",
    "\n",
    "answer,elapsed_time = Mistral_Infer(query)\n",
    "\n",
    "print(\"[Question]:  \",query,'\\n\\n',\"[Answer]:  \",answer,'\\n\\n',\"[Ref_Answer]:  \",ref_answer)\n",
    "print(\"\\nInference time = \", elapsed_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91049fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
