{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation     \n",
    "   \n",
    "   \n",
    "This notebook provides a comprehensive guide for synthetic data generation using self-instruct framework, to prepare training and validation datasets for Supervised Fine Tuning (SFT) an LLM. \n",
    "\n",
    "In this notebook, we use the self-instruct framework to process a document in HTML format and generate question-answer pairs by LLM on Amazon Bedrock. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db66088",
   "metadata": {},
   "source": [
    "### Step 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa54ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install the required Python packages \n",
    "\n",
    "#!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "#!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "#!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ee1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a51028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Bedrock in AWS region\n",
    "boto3_bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "boto3_bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strings_recursive(test_str, tag):\n",
    "    try:\n",
    "        # finding the index of the first occurrence of the opening tag\n",
    "        start_idx = test_str.find(\"<\" + tag + \">\")\n",
    "\n",
    "        # base case\n",
    "        if start_idx == -1:\n",
    "            return []\n",
    "\n",
    "        # extracting the string between the opening and closing tags\n",
    "        end_idx = test_str.find(\"</\" + tag + \">\", start_idx)\n",
    "        res = [test_str[start_idx+len(tag)+2:end_idx]]\n",
    "\n",
    "        # recursive call to extract strings after the current tag\n",
    "        res += extract_strings_recursive(test_str[end_idx+len(tag)+3:], tag)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    except:\n",
    "        return \"bad format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1814ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_Gen_Bedrock(model_id,model_kwargs,prompt):\n",
    "                \n",
    "    input_token = len(prompt.split())/0.75\n",
    "\n",
    "    if ('titan' in model_id):    \n",
    "        model_body = {\n",
    "            \"inputText\": f\"{prompt}\"\n",
    "        }\n",
    "        model_body[\"textGenerationConfig\"] =  model_kwargs  \n",
    "    elif ('claude-3' in model_id):\n",
    "        model_body = {\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"max_tokens\": 1024,\n",
    "                        \"messages\": [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                            }\n",
    "                        ],\n",
    "        }\n",
    "    else:\n",
    "        model_body = {\n",
    "            \"prompt\": f\"{prompt}\"\n",
    "        }\n",
    "        model_body.update(model_kwargs)\n",
    "\n",
    "    body_bytes = json.dumps(model_body).encode('utf-8')\n",
    "\n",
    "    st = time.time()\n",
    "\n",
    "    if ('claude-3' in model_id):\n",
    "        response = boto3_bedrock_runtime.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    body=body_bytes,\n",
    "                )\n",
    "    else:\n",
    "        response = boto3_bedrock_runtime.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    contentType=\"application/json\",\n",
    "                    accept=\"*/*\",\n",
    "                    body=body_bytes,\n",
    "                )\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    if ('titan' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"results\"][0][\"outputText\"].strip()\n",
    "        llm_latency = response[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amzn-bedrock-invocation-latency\"]\n",
    "    elif ('llama' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"generation\"].strip()\n",
    "    elif ('claude-v2' in model_id or 'claude-instant-v1' in model_id ):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"completion\"].strip()\n",
    "    elif ('claude-3' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"content\"][0][\"text\"].strip()\n",
    "    elif ('mistral' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"outputs\"][0][\"text\"].strip()    \n",
    "    else :\n",
    "        llm_response = 'MODEL TYPE NOT YET SUPPORTED.'\n",
    "    \n",
    "    output_token = len(llm_response.split())/0.75\n",
    "\n",
    "    throuput = output_token/elapsed_time\n",
    "    \n",
    "    return llm_response, elapsed_time, input_token, output_token, throuput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb677037",
   "metadata": {},
   "source": [
    "### Step 1: Generate seed q-a pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec46e8",
   "metadata": {},
   "source": [
    "Create prompt template to generate seed questions, one question for each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qagen = \"\"\"\n",
    "<s>[INST] \n",
    "You are an AI assistant, your task is to generate question-answer pair from the given context. \n",
    "\n",
    "Analyze the context within the <context> XML tag, generate one question from the context. \n",
    "In the question, DO NOT refer to the context.  \n",
    "provide answer to each question according to the content in the context. \n",
    "In your response, present the question within the <question> tag, and the answer within the <answer> tag.\n",
    "DO NOT nest <question> and <answer> element. \n",
    "DO NOT put any extra attribute in the <question> and <answer> tag. \n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "[/INST] </s>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_qagen = PromptTemplate(template=prompt_template_qagen, input_variables=[\"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e009c",
   "metadata": {},
   "source": [
    "Load and process sections in HTML format using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d760e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your HTML document in the ./data and specify the full path below \n",
    "\n",
    "html_file = <your document.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(html_file)\n",
    "data = loader.load()\n",
    "pages = loader.load_and_split()\n",
    "print(\"The documents contain \"+str(len(pages))+\" pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0aac7",
   "metadata": {},
   "source": [
    "Generate question-answer pair using Mistral-7b-instruct model on Bedrock. You can choose a different model and configure LLM hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b50872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mistral.mistral-7b-instruct-v0:2' \n",
    "\n",
    "model_kwargs = {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.05\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "context_list = []\n",
    "question_list = []\n",
    "answer_list = []\n",
    "\n",
    "for k in range(len(pages)):\n",
    "    print(\"\\nPage \",k+1)\n",
    "    docs = pages[k].page_content  \n",
    "    #print(docs)\n",
    "\n",
    "    paragraphs = docs.split('\\n\\n')\n",
    "\n",
    "    for text in paragraphs:\n",
    "        if len(text)>10:\n",
    "            print(i+1,end=': ')\n",
    "            prompt = PROMPT_qagen.format(context = text)\n",
    "            #print(prompt)\n",
    "\n",
    "            qa_response = QA_Gen_Bedrock(model_id,model_kwargs,prompt)\n",
    "            #print(qa_response[0])\n",
    "            \n",
    "            res_q = extract_strings_recursive(qa_response[0], \"question\")[0]\n",
    "            res_a = extract_strings_recursive(qa_response[0], \"answer\")[0]\n",
    "            \n",
    "            #if \"bad format\" in res_q or \"bad format\" in res_a:\n",
    "            if \"bad format\" in res_q or \"bad format\" in res_a or len(res_q)==0 or len(res_a)==0:\n",
    "                pass\n",
    "            else:\n",
    "                context_list.append(text)\n",
    "                question_list.append(res_q)\n",
    "                answer_list.append(res_a)\n",
    "                i=i+1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "print(\"\\nCompleted...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f71421",
   "metadata": {},
   "source": [
    "Save the seed question-answer pairs in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_data_file = html_file+\"_seed_selfinstruct_mistral-7b.csv\"\n",
    "\n",
    "df_train_dataset = pd.DataFrame()  \n",
    "\n",
    "df_train_dataset[\"context\"] = context_list\n",
    "df_train_dataset[\"question\"] = question_list\n",
    "df_train_dataset[\"answer\"] = answer_list\n",
    "\n",
    "df_train_dataset.to_csv(seed_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd979b27",
   "metadata": {},
   "source": [
    "### Step 2: Human validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a2fd6",
   "metadata": {},
   "source": [
    "Leverage the domain SMEs' expertise to validate the seed question-answer pairs in Step 1, and update the dataset to get ready for the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c1881",
   "metadata": {},
   "source": [
    "### Step 3: Generate training and validation dataset from seed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0c17b",
   "metadata": {},
   "source": [
    "Create prompt template for generating more question-answer pairs from the seed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584cae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_trngen = \"\"\"\n",
    "<s>[INST]\n",
    "You are an AI assistant, your task is to generate question-answer pair from the given context. \n",
    "\n",
    "Analyze the context within the <context> XML tag and the seed question in <seed> XML tag, \n",
    "generate four questions that rephrases the seed question within the <seed> XML tag. \n",
    "Make sure the generated questions are also relevant to the context within the <context> XML tag. \n",
    "\n",
    "In your response, present the question within the <question> tag.\n",
    "DO NOT nest <question> element. \n",
    "DO NOT put any extra attribute in the <question> tag. \n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<seed>\n",
    "{seed_question}\n",
    "</seed>\n",
    "\n",
    "[/INST] </s>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_trngen = PromptTemplate(template=prompt_template_trngen, input_variables=[\"context\",\"seed_question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299a55d",
   "metadata": {},
   "source": [
    "Generate more question-answer pairs to scale up the training and validation datasets for LLM fine-tuning   \n",
    "\n",
    "Here for each seed question-answer pair, we generate    \n",
    "- 3 additional q-a pairs for training data\n",
    "- 1 additional q-a pair for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df571b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data \n",
    "df_input = pd.read_csv (seed_data_file)\n",
    "context_list = df_input.context.values.tolist()\n",
    "question_list  = df_input.question.values.tolist()\n",
    "answer_list  = df_input.answer.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b606c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate question-answer pairs \n",
    "trn_context_list = []\n",
    "trn_seed_list = []\n",
    "trn_question_list = []\n",
    "trn_answer_list = []\n",
    "\n",
    "val_context_list = []\n",
    "val_seed_list = []\n",
    "val_question_list = []\n",
    "val_answer_list = []\n",
    "\n",
    "for i in range(len(question_list)):\n",
    "\n",
    "    trn_context_list.append(context_list[i])\n",
    "    trn_seed_list.append(question_list[i])\n",
    "    trn_question_list.append(question_list[i])\n",
    "    trn_answer_list.append(answer_list[i])    \n",
    "    \n",
    "    print(i+1,end=': ')\n",
    "    prompt = PROMPT_trngen.format(context = context_list[i], seed_question = question_list[i])\n",
    "    #print(prompt)\n",
    "\n",
    "    qa_response = QA_Gen_Bedrock(model_id,model_kwargs,prompt)\n",
    "    #print(qa_response[0])\n",
    "\n",
    "    res_q = extract_strings_recursive(qa_response[0], \"question\")\n",
    "\n",
    "    #if \"bad format\" in res_q or \"bad format\" in res_a:\n",
    "    if \"bad format\" in res_q or \"bad format\" in res_a or len(res_q)==0 or len(res_a)==0:\n",
    "        pass\n",
    "    else:\n",
    "        if (len(res_q)>3):\n",
    "            num_q = 3\n",
    "        else:\n",
    "            num_q = len(res_q)\n",
    "            \n",
    "        for j in range(num_q):\n",
    "            trn_context_list.append(context_list[i])\n",
    "            trn_seed_list.append(question_list[i])\n",
    "            trn_question_list.append(res_q[j])    # 3 for trn \n",
    "            trn_answer_list.append(answer_list[i])\n",
    "            print('.',end='')\n",
    "        \n",
    "        val_context_list.append(context_list[i])\n",
    "        val_seed_list.append(question_list[i])\n",
    "        val_question_list.append(res_q[3])        # 1 for val\n",
    "        val_answer_list.append(answer_list[i])\n",
    "        print('*',end='')\n",
    "        \n",
    "print(\"\\nCompleted: generated \", len(question_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695f060",
   "metadata": {},
   "source": [
    "Save the training and validation datasets in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = html_file+\"_trnfromseed_mistral-7b.csv\"  \n",
    "\n",
    "df_trn_dataset = pd.DataFrame()  \n",
    "\n",
    "df_trn_dataset[\"context\"] = trn_context_list\n",
    "df_trn_dataset[\"seed_question\"] = trn_seed_list\n",
    "df_trn_dataset[\"question\"] = trn_question_list\n",
    "df_trn_dataset[\"answer\"] = trn_answer_list\n",
    "\n",
    "df_trn_dataset.to_csv(TRN_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FILE = html_file+\"_valfromseed_mistral-7b.csv\"  \n",
    "\n",
    "df_val_dataset = pd.DataFrame()  \n",
    "\n",
    "df_val_dataset[\"context\"] = val_context_list\n",
    "df_val_dataset[\"seed_question\"] = val_seed_list\n",
    "df_val_dataset[\"question\"] = val_question_list\n",
    "df_val_dataset[\"answer\"] = val_answer_list\n",
    "\n",
    "df_val_dataset.to_csv(VAL_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138eb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65e2957",
   "metadata": {},
   "source": [
    "### Step 4: Semantic verification \n",
    "      \n",
    "Calculate Semantic Similarity Score between seed dataset and training/validation dataset. Here we use Titan-text-embedding-v1 model to calculate, you can choose other embedding models on Bedrock or from 3rd party.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cec1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# SS using Titan embedding model\n",
    "def get_titan_embedding(text):\n",
    "    \n",
    "    body = json.dumps({\"inputText\": text})\n",
    "    modelId = 'amazon.titan-embed-text-v1'    # support 8K token \n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'    \n",
    "    \n",
    "    response = boto3_bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    \n",
    "    return embedding\n",
    "    \n",
    "def calculate_semantic_sim_titan(pred_list,ref_list):\n",
    "   \n",
    "    sem_score = []\n",
    "    average_sem_sim = 0\n",
    "    \n",
    "    for i in range(len(ref_list)):\n",
    "        #print(i,\" \",end = ':')\n",
    "        ref_embedding = get_titan_embedding(ref_list[i])\n",
    "        pred_embedding = get_titan_embedding(pred_list[i])\n",
    "        cos_sim = util.cos_sim(ref_embedding, pred_embedding)\n",
    "        #print(cos_sim[0][0].item())\n",
    "        \n",
    "        sem_score.append(cos_sim[0][0].item())\n",
    "    \n",
    "    average_sem_sim_titan = np.mean(sem_score)   \n",
    "    \n",
    "    #print(\"Average similarity: \", average_sem_sim)\n",
    "    \n",
    "    return average_sem_sim_titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = html_file+\"_trnfromseed_mistral-7b.csv\"  \n",
    "\n",
    "df_trn = pd.read_csv (TRN_FILE)\n",
    "trn_seed_list = df_trn.seed_question.values.tolist()\n",
    "trn_question_list = df_trn.question.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ea09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FILE = html_file+\"_valfromseed_mistral-7b.csv\"  \n",
    "\n",
    "df_val = pd.read_csv (VAL_FILE)\n",
    "val_seed_list = df_val.seed_question.values.tolist()\n",
    "val_question_list = df_val.question.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ss_score = calculate_semantic_sim_titan(trn_seed_list,trn_question_list)\n",
    "trn_ss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ss_score = calculate_semantic_sim_titan(val_seed_list,val_question_list)\n",
    "val_ss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f13a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9031e599",
   "metadata": {},
   "source": [
    "### Step 5: LLM Fine Tuning using the generate datasets\n",
    "      \n",
    "Please continue this step with the \"fine_tuning_self-instruct_notebook\".   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a57840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
